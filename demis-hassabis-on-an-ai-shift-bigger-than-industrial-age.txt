So good to see you. Good to see you, too. It was in San Francisco. Yes, I know.

You are all over the place. I'm curious if this year feels different than the last time you were in Davos. Gemini three is out. We've heard that opening.

I called it a code red internally. Do you feel like Google got its mojo back? Well, I'm not sure if that's for me to say, but I feel like we had a very good year. So it's been hard work, really hard work getting our technology and the models sort of to back to state of the art.

I think we did that with Gemini three especially and nanobots on our imaging software. And then I think we've also sort of adapted really to this new world of shipping very fast. Kind of bringing a kind of startup energy to what we do. Do you think people underestimated Google or got something wrong?

Yes, maybe. I'm not sure. I mean, I think we always had the ingredients, you know, to to to be at the forefront of this. Obviously, we've got the long history in it.

I think, you know, over the last decade, Google and DeepMind, between us, we've invented most of the breakthroughs that the modern A.I. industry relies on. Now see Transformers, most famously by AlphaGo Deep reinforcement learning these things. And we have these incredible product surfaces billions user services.

The natural fit for A.I. actually from search to email to to to chrome and but it's just getting all of that together and organized in the right way. And I think we've done that in the last couple of years, and there's still a lot more work to do, but I think we're starting to see the fruits of that. If you think you have an advantage, how big do you think your advantage is?

How long does it last? Well, I think everything starts with research, in my opinion. And, you know, the models, especially being at the state of the art on all the different benchmarks. And that's what we focused on first when we put Google and DeepMind together.

And I think with the Gemini series, you're very happy with how that's going. There's a lot more work to do there. But I think we're the only organization that has the full stack from the TTPs and the and the hardware, the data centers, the cloud business, the Frontier lab, and all of these amazing products that can, you know, kind of natural fits for A.I.. So really, structurally from first principles, we should be doing very well.

And I think there's actually a lot more headroom to come from here. I wonder what a day in the life of a frontier model leading ACO is like. Like I've read that you do most of your thinking between one and four. AM Yes, that's right.

Is it ever not a code red inside? Like, do you ever feel comfortable? No. You never feel comfortable.

I mean, we try, you know, code red zone for very special circumstances. But it's always I mean, for the last I would say three, four years, it's been unbelievably intense. And, you know, 100 hour weeks, 50, you know, 50 weeks a year. And that's the norm.

And I think that's what you have to do at the forefront of this unbelievably fast moving technology. It's ferociously competitive out there. Maybe the most intense competition has ever been in technology. And the stakes are incredibly high.

AGI and, you know, all of that, that that can mean so commercially, scientifically. And then if you add all the excitement of what we're doing and, you know, using it as my passion, as you know, is exploring scientific, you know, problems with AI, accelerating scientific discovery itself. This is what I've always dreamed about and I've worked my whole life on A.I. towards this moment.

So it's sort of hard to sleep because there's so much work to do. But also, it's it's it's there's so much exciting things to to look into and to to push forward. I mean, I know you're very focused on driving scientific progress, just discovering new materials. Even we've seen now Gemini being integrated into humanoid robots.

Is the alpha fold moment for the physical world and I here. What is that and what does that look like? Yeah, I spent a lot of the last year actually looking very carefully into robotics. I do think we're on the cusp of a kind of breakthrough moment in physical intelligence.

I still think we're about 18 months, two years away from doing. We need to do more research. But I think the foundation models like Gemini show the way forward. I mean, from the beginning we made Gemini multimodal so you could understand the physical world for multiple reasons.

One was so we could build a universal assistant. Maybe that exists on your glasses or your phone that understands the world around you. But of course, a second use of that would be for robotics. So what is this, that moment for the physical world look like?

I think it's it's having robots, you know, reliably do useful tasks out in the world. And I think there's a few things holding that back. Still. Part of it is the algorithms are still not quite there.

They need a little bit more robustness. They have to work with less data than you get for the labs or the models that work on just digital. Well, you can sort of create synthetic data a lot harder to make that kind of data. And there's still sort of some problems in the hardware that are not solved specifically things like the arm and the hand.

Actually, when you look into robotics very carefully, you get a newfound appreciation, at least I did, for the human hand, and how exquisite evolution is design that it's incredible and it's very hard to match the reliability, the strength and the dexterity that the human hand has. So there's still quite a lot, I, in my opinion, pieces that put together. But there's very exciting things. I mean, we just announced a new deep collaboration with the Boston Dynamics.

They've got some very exciting robots and end die and actually applying it to automotive manufacturing. And we'll see over the next year how that goes in sort of prototype form. And then maybe in a year or two we'll have some really impressive demonstrations that we can scale up. A year ago, deep sea seemed cataclysmic for the West.

Now, a year later, it's it's quiet. China seems to have been quieter. Yes. Has your opinion on competition from China changed?

Not really. I mean, I didn't think it was cataclysmic in the first place. I think it was a massive overreaction in the West. It was impressive.

And I think it shows that the Chinese are very capable that the leading companies I think companies like Bytedance actually, I would say are the most capable and they're maybe only six months behind, not one or two years behind the frontier. So I think that's what they showed. Some of the claims were overexaggerated about, you know, the amount of compute they used and being so minimal and so on, because they relied on some Western models and also fine tuning on the outputs of some of the leading Western models. So it wasn't sort of denied.

MALVEAUX And the other thing I think so far is not yet to be seen is can China actually the Chinese companies innovate beyond the frontier themselves so that that, you know, they're gaining, that they're very good at kind of catching up to where the frontier is and increasingly capable of that. But I think they've yet to show they can innovate beyond the frontier. You helped define AGI. You have said we have a 50% chance of getting there by 2030.

Is that still your timeline? It is. And is AGI still a useful target for you? I think so.

I think it's a very good list in my timeline, and it's a little bit longer than some others that your observation is that you're here. But my bar is quite high. It's it's the it's the ability to, you know, a system that exhibits all the cognitive capabilities humans have. And I think we're still clearly quite far from that.

That means, you know, things in like scientific creativity, not just solving a conjecture or solving a problem in science, but actually coming up with the hypothesis or the problem in the first place. And as any scientist knows, finding the right question is actually often way harder than finding the answer. So I don't you know, it's not clear that that these systems have that capability that they definitely don't right now. I think they will eventually, but it's not clear what's needed still.

And as things like continue learning, you know, online learning, going beyond what they've been sort of trained for, and then they're static out in the world, they need to learn on the fly. So there's quite a few, in my view, missing capabilities that are quite critical to what I would regard as an AGI system. Google's a major investor, and Anthropic Del Rio was here earlier today. Do you agree or disagree with his prediction that I will wipe away 50% of entry level white collar jobs in five years?

I think it's that's also my time and my view on that would be a lot longer. I mean, I think we're starting to see maybe the beginnings of that this year in terms of maybe entry level jobs or internships, those types of things. But I think there's we would have to solve a lot more of this consistency that doesn't have Right. And I call it jacket intelligence, as we're very good at certain things and it's very poor.

The current systems are other things. And if you want to offload or delegate an entire task to say an agent, rather than having what we have today, which are more like assistive programs, you're going to need a lot more consistency across the board. It's no good for it to be good at 95% of that task. You need it to be good at the whole task for you to be able to actually just sort of fire and forget on it.

So I think this there's there's still quite a lot more to be done before we'll see that kind of disruption. But that kind of disruption will happen. I think eventually, sure. I mean, in the limit with AGI, you know, if you have systems like that, I think that changes the whole economy.

But way beyond the question of jobs, I think potentially if we build it right, we we're in a post scarcity world where we solve some of the kind of fundamental nodes of the world like energy sources, new clean, renewable, basically free energy sources. If we solve fusion, something like that, with the help of A.I. new materials, I think we'll be, you know, five, ten years past. AGI will be in a in a radical abandoned world.

And so what does that mean with, you know, to the economy and and how how society works? Actually, before we get to a post scarcity world, though, if we get there, there is so much anxiety about what happens in between. You know, I'm a mom. I know you have kids.

Like what scares you most for them? What do you talk to them about? What do you tell them that's coming? I mean, I've just heard so many.

Oh, my gosh. For college graduates are going to have such a hard time. Well, I don't know about that. Look, I think it's it's it's going to be an age of disruption, just like the Industrial revolution was maybe ten acts of that, which is kind of unbelievable to think about ten times faster than I usually describe it.

It's going to be ten times bigger and ten times faster. The Industrial Revolution, two years, 100 now get. So 100 X of it. No, I say this to everyone, but I think that comes with it.

Huge opportunities. And I also am just a big believer in human ingenuity. We're extremely adaptable because our minds are so general. You know, the human mind is very general.

We've adapt to look at the modern world around us. We, you know, our hunter gatherer minds have managed to build modern civilization. So I think we'll adapt again. I think it's a little bit unprecedented because of the speed of it.

Usually it takes one generation or two generations for a transformation like this to happen and the magnitude of the transformative power of this technology. But I think the kids today, you know, I'd be encouraging them to get incredibly proficient with these new tools and native with them, and they're almost equivalent of giving them superpowers, you know, in the creative arts that you could probably do the job of, you know, what would have taken ten people in one. And I think that means, you know, if you entrepreneurial, if you're creative with game design films, projects, you can probably get a lot more done and break into. Those industries a lot more easily than you could in the past, as you know.

A new upcoming talent. Some folks have advocated for a pause to give regulation time to catch up, to give society time to sort of adjust to some of these changes. In a perfect world, you knew that every other company would pause. If every country would pause.

Would you advocate for that? I think so. I mean, I've been on record saying what I'd like to see happen. It was always my dream of the kind of the road map at least I had when I started out the mind 15 years ago and start working in, you know, 25 years ago now, was that as we got close to this moment, this threshold moment of AGI arriving, we would maybe collaborate, you know, in a scientific way.

I sometimes talk about setting up an international CERN equivalent for AI where all the best minds in the world would collaborate together and do the final steps in a very rigorous scientific way involving all of society, maybe philosophers and social scientists and economists, as well as technologists to kind of figure out what we want from this technology and how to utilize it in a in a in a way that benefits all of humanity. And I think that's what's at stake. Unfortunately, it kind of needs international collaboration, though, because even if one company or even one nation or even the West decided to do that, it's the has no use unless the whole world agrees, at least on some kind of minimum standards. And, you know, international cooperation is a little bit tricky at the moment.

So that's going to have to change if we want to, to to have that kind of rigorous scientific approach to the final steps to AGI. So if AGI comes in, let's say 2030, we don't have the regulations set up yet. Are we destined for something difficult? Well, then where you know, I doesn't I think I'm still optimistic that enough of the leading players will kind of communicate together and hopefully collaborate at least on things like safety and security protocols.

There's a lot of that already. We work quite closely with Anthropic, for example, on those things, and that would be needed then, you know, maybe kind of more peer based cooperation if we can't get that international thing to work. But I would, you know, that would be a lot more like Sam to cooperate with you. I potentially I'm you know, I think I'm on pretty good terms with pretty much all the other leaders of all the leading labs.

I mean, I think if the stakes are high enough and I think a lot of it is understanding what's at stake and what the risks are. And I think that will become clearer to everyone in the next two or three years. So let's talk about the technology and the next curve. Yang Coombs has said he doesn't think Transformers and lamps alone will get us to AGI.

Do you agree or disagree? And and, you know, if they're dead ends and what are we doing? Yes. No, I disagree that they're dead ends.

But obviously, I think that's clearly wrong. I mean, they're so amazingly useful already. But I think the way I say is an empirical question. I think it's a scientific question whether they're going to be enough on their own.

I think it's a 5050 that, you know, just scaling up existing methods with some tweaks will be enough. It might be. And you have to do that. And I think that's useful work because at a minimum, the way I look at it is these lambs will be a component, a massively important component of the final system.

The only question in my mind is, is it is it the only component? Right. And I could imagine there are one or two breakthroughs, maybe a small handful, you know, less than five that still need it from here, right? Yes.

And so, you know, and those might be things like world models. That's something I've talked about we're working on. In fact, we have the best world model currently, which is GENI, our GENI system. And I work on that directly and I think it's very important.

But also things like continue learning and having consistent systems that are, you know, don't have these jagged edges that they're good at and not good at. That's a general system. Shouldn't have that. So I think, you know, better reasoning, more long term planning, there's quite a few capabilities that are still missing.

And it's an open question whether a new architecture or new breakthrough is needed or more of the same. And we're just, from my point of view, from Google, Deepmind's point of view, we're pushing as hard as possible. And both those things, both inventing new things and scaling up existing things, so slightly different but related. Elias Escovedo said the era of scaling and making bigger models make improvements is is nearly over.

Is that something you agree? I don't agree. I think that his exact quote was this We're back to the age of research. But but and you know, I love Ellie and we're very good friends and we agree on a lot of things.

But my view is we never left the age of research, at least from the point of view of the point. We've always invested. We've in my view, we've always had the the deepest and broadest bench, actually Google and DeepMind together we have. The last decade, we've invented about 90% of the breakthroughs that, you know, the modern industry relies on.

Of course, Transformers, most famously, but also, you know, deep reinforcement learning. AlphaGo these kinds of reinforcement learning techniques. We pioneered all of that. So if some new breakthroughs are required in the future, I would back us to be the ones just like in the past, to be the ones to make those breakthroughs, you know, in the future.

Last, agree or disagree, Elon says we have entered the singularity. No, I think that's very premature. You know, I think the singularity is another word for, you know, a full AGI arriving. And I think I explained earlier why I think that we're still, you know, nowhere near that.

I think we will get there. And, you know, five years, even five years is not a long amount of time if you think about what that is. But I think there's still a lot of work to be done before we have anything that looks like the singularity. So talk to us a little bit about the culture inside Google right now to, you know, win this race, but do it right.

You know the leadership. How involved are Larry and Sergey right now? How often do you talk to them and what are their priorities? Yeah, they're very involved in.

And, you know, Larry, more on the strategic side. You know, I see him at board meetings and other times when I visit the valley. So, again, it's more hands on on the you know, he's involved in the coding of, you know, on the Gemini team specifically more in the in the in the algorithmic details. And it's great having them both energized around where we are and who wouldn't be, you know, this moment Absolutely incredible moment for computer science.

So just from a scientific point of view, which both of them are, it's an incredibly exciting moment in history, human history, really. And so, of course, everyone wants to be hands on and heavily involved in that. So that's great. And and for us, just as as a as an entity, you know, I'm trying to combine the best of many worlds.

So, you know, startup energy of shipping things fast and taking risks and doing things like that, which I think you're seeing the benefits of, you know, the big company resources is amazing, useful, but also protecting the space for long term research still, and exploratory research, not just researching what will deliver in, you know, three months in a product. I think that would be a mistake, too. So I'm trying to balance all of those different factors together. And, you know, I think in the last year things have been going well and I think we can still do better and I think we still will do better this year.

But I'm very happy with our trajectory. I think it's the steepest of actually of improvement and progress of anyone in the industry. You are a Nobel laureate and I know how obsessed you are with AI powering scientific research. If I itself, let's say, makes a Nobel worthy discovery, you should get the prize.

Yeah, I are the human, I think still human, I would say, because I feel like I mean, it depends what you mean by completely on its own right. So for now, these are still tools. And I view them as, you know, like very maybe the ultimate scientific tool, but sort of better versions of telescopes and microscopes. We've always built tools so that we can investigate the natural world better.

We're toolmaking animals, basically. That's what distinguishes humans from from the other animals. And that's how superpower and I include computers in that, of course. And A.I.

being the ultimate expression of that. So in some ways, I think of A.I., and I've always thought of it as the ultimate tool to do science. And and I think for the foreseeable future, that's going to be a collaboration with top scientists, putting in the creative ideas and maybe the hypotheses with these amazing tools that enhance, you know, data processing and pattern matching and the exploration of science. You you obviously could have sold DeepMind to anyone.

And I think all of these companies are asking a lot of us to trust. Yes. Especially if regulation doesn't keep up with technology, which history shows it probably won't. Why?

Why should we trust you? And why do you think Google, which I would implicitly think you believe is is the the place that we should believe in the most? Well, comes to something that feels so risky. Yes.

I think you need to judge these companies by their actions and also look into, you know, the motivations, I would say, of the leaders involved in those you know, those endeavors. And for me and for us. And that's one reason I picked Google as the right home for DeepMind is several reasons, the main one being that the founders of Google and the way Google was set up by them was as a scientific company. You know, many people forget Google itself was a Ph.D.

project, right? It was Larry and Sergey project. So I felt an immediate affinity with with them. And Larry led the acquisition, but also the board who they collected in the board.

You know, you have John Hennessy's, the chair, who's a Turing Award winner himself, and Francis Arnold, another Nobel Prize winner. You know, these are unusual people to have on a corporate board. So the whole environment is very scientific and science. Science and research led and engineering led as a culture and is deeply ingrained in the culture.

And that means doing science at the highest level means being really rigorous, being thoughtful, and applying the scientific method everywhere you can. I think that's not just to the technology, but it's also to the way you operate as an organization. So I feel like, you know, we we're very we tried to be very thoughtful and responsible and have as much force as possible over the technologies we put out in the world. Doesn't mean we'll get everything right because it's so complex and so new and nascent and so transformative, this technology.

But we hope to coast correct as quickly as possible if there, you know, something does go wrong. And then the final thing I would say is just the I was just attracted by the the types of things Google tries to do in the world. You know, organizing the world's information, I think is a very noble goal, which is obviously Google's mission statement. And I think it fitted very well with that, with Deepmind's mission statement of solving intelligence and using it to to solve everything else.

And and that was our and I think those two mission statements are natural fit AI and organizing the world's information naturally go together. And I think those are the types of products, the types of products that Google is well known for, for maps to 2 to 2 Gmail to obviously search. I think they're genuinely useful products in the world, and I think AI is an easy fit. How that would work with those products to enhance them for everybody to use in their everyday lives.

And I think that's a great thing for the world. So, you know, I'm happy to be contributing to that. Okay, so post scarcity world where there people no longer have jobs, what do you personally plan to do with your time once you've achieved all your technical goals? Yes.

Research is just automating itself, right? Well, I would love to use it for what I will do. Post the Singularity is to use it for exploring the limits of physics. I think that was my my favorite subject at school was the big questions.

You know, what is the fabric of reality? What's the nature of reality? What about the nature of consciousness? The answer to the Fermi paradox, all of these things.

What is time? What is gravity? I'm amazed more people, you know, we just go around our daily lives, not really thinking about these massive questions that for me are always kind of almost screaming at me for like, what is the answer to these things, these deep mysteries? And I would like to use AI and to explore all of those things, maybe traveling to the stars as well, with the help of, you know, and new energy sources and materials and other things that's unlocked by will we all have meaning and purpose if we don't have work?

Well, I think that's the to be honest with you, that's the thing I worry more about than the economics. I think the economics is almost a political question of like when we get all of these extra benefits and productivity, can we make sure that it's shared for the benefit of everyone? And I think obviously that's what I believe in. But then the bigger question, and that is what about purpose and meaning that a lot of us get from our jobs in scientific endeavors?

How will we find that in the new world? And I think, you know, we will need some new great philosophers, in my opinion, to to help with that and thinking that through maybe will, you know, be get much more sophisticated with our art and and exploration that we do and things like, you know, extreme sports as many things we do today that aren't just for economic gain. And perhaps we'll have very esoteric versions of those things in the future. All right.

So everyone in the room is wondering what they should be doing. Like, what should I do about what do I do sitting here in Davos in ten years? What is the biggest mistake, do you think people in this room will have made about AI? Well, look, I think there's two things I would say.

One is for the younger generation and our kids and so on is the only thing we're certain of is there's going to be a huge amount of change. So I think in terms of learning skills, get ready to because kind of learning to learn is the most important thing. How can quickly can you adapt to new situations, absorb new information using the tools that we have for the CEOs and and business folks in the room? I think the most important thing to do now is there are many providers of leading models and leading service providers, and they'll be more for these A.I.

models, you know, pick the partners that you feel are approaching it in the right way. And so, you know, kind of partner with those that are making the changes and approaching this technology in the way that you would like to see in the world. And I think together we can kind of build that future with, you know, A.I. coming down the line that we will want.
